{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter data analysis through Spark Streaming\n",
    "\n",
    "This is my first project with Spark Streaming and I decided to stream and analyze twitter data. Twitter data is a rich source of information about any topics. This data can be used in different use cases such as finding trends related to a specific keyword, focus on users on specific location,targeting specific device, targeting people with same interest etc\n",
    "\n",
    "I found this article helped me most in figuring out how to extract, filter, and process data from twitter api.\n",
    "\n",
    "In this project, I am uing Twitter data to do the following analysis.\n",
    "\n",
    "* Compare the popular hashtag words\n",
    "* Compare the number of tweets based on Country\n",
    "* Compare the popularity of device used by the user for example iphone, android etc.\n",
    "\n",
    "In the first part, we will see how to connect to Twitter Streaming API and how to get the data. In the second part, will see how to structure the data for analysis and in the last pat, will see how to Visualize our data.\n",
    "\n",
    "# Getting Data stream from Twitter\n",
    "\n",
    "In order to access Twitter Streaming tweets following steps need to be completed.\n",
    "* We need to create a twitter account and register on TwitterApps.\n",
    "* Go to your newly created app and generate Keys and Access Tokens.\n",
    "* Copy your Twitter app credentials, API key, API secret, Access token and Access token secret.\n",
    "\n",
    "## Part 1. Connecting to Twitter Stream to get tweets\n",
    "\n",
    "This simple program will get the tweets from Twitter API using Python and passes them to the Spark Streaming instance.\n",
    "\n",
    "Let’s create a file called connect_app.py with the following code.\n",
    "\n",
    "Import the libraries that we’ll use. Connect to twitter and start streaming the tweets. Twitter API gives us a lot of data about each tweet in a json data structure. We are interested in several parts of tweet that can be accessed with the key [\"put the key\"]. we do capture the tweet = json.loads(line) statement inside a try/except clause to fecilitate the script to skip over a line of bad data and continue processing the remaining tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "import sys\n",
    "import requests\n",
    "import requests_oauthlib\n",
    "import json\n",
    "import bleach\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# Include your Twitter account details\n",
    "ACCESS_TOKEN = '838123377525153793-hcHH4ZxCCIZLX3VyoY5q5tNxTe9Rx19'\n",
    "ACCESS_SECRET = 'okxo1grjZRnvpcwYsN0AeOzvOQdMDoIrP7l7qUgzcdMwM'\n",
    "CONSUMER_KEY = 'QDgoiUYXDMmn1fyCBCxcYsBfc'\n",
    "CONSUMER_SECRET = '8puNh3Lp6rM6pNZ2lvgkA5uYwbBXDUFGtwWxsS7sX1dCO8gDiN'\n",
    "my_auth = requests_oauthlib.OAuth1(CONSUMER_KEY, CONSUMER_SECRET,ACCESS_TOKEN, ACCESS_SECRET)\n",
    "\n",
    "\n",
    "def get_tweets():\n",
    "  url = 'https://stream.twitter.com/1.1/statuses/filter.json'\t\n",
    "  query_data = [('language', 'en'), ('locations', '-130,-20,100,50'),('track','iphone')]\n",
    "  query_url = url + '?' + '&'.join([str(t[0]) + '=' + str(t[1]) for t in query_data])\n",
    "  response = requests.get(query_url, auth=my_auth, stream=True)\n",
    "  print(query_url, response)\n",
    "\n",
    "  return response\n",
    "\n",
    "def send_tweets_to_spark(http_resp, tcp_connection):\n",
    "  for line in http_resp.iter_lines():\n",
    "    try:\n",
    "      full_tweet = json.loads(line)\n",
    "      tweet_text = full_tweet['text']\n",
    "      print(\"Tweet Text: \" + tweet_text)\n",
    "      print (\"------------------------------------------\")\n",
    "      tweet_screen_name = \"SN:\"+full_tweet['user']['screen_name']\n",
    "      print(\"SCREEN NAME IS : \" + tweet_screen_name)\n",
    "      print (\"------------------------------------------\")\n",
    "      source = full_tweet['source']\n",
    "      soup = BeautifulSoup(source)\n",
    "\n",
    "      for anchor in soup.find_all('a'):         \n",
    "        print(\"Tweet Source: \" + anchor.text)        \n",
    "\n",
    "      tweet_source = anchor.text\n",
    "      source_device = tweet_source.replace(\" \", \"\")\n",
    "      device = \"TS\"+source_device.replace(\"Twitter\", \"\") \n",
    "      print(\"SOURCE IS : \" + device)\n",
    "      print (\"------------------------------------------\")\n",
    "      tweet_country_code = \"CC\"+full_tweet['place']['country_code']\n",
    "      print(\"COUNTRY CODE IS : \" + tweet_country_code)\n",
    "      print (\"------------------------------------------\")\n",
    "      tcp_connection.send(tweet_text +' '+ tweet_country_code + ' '+ tweet_screen_name +' '+ device +'\\n')\n",
    "\n",
    "    except:\n",
    "      continue\n",
    "   \n",
    "\n",
    "TCP_IP = 'localhost'\n",
    "TCP_PORT = 9009\n",
    "conn = None\n",
    "s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "\n",
    "s.bind((TCP_IP, TCP_PORT))\n",
    "s.listen(1)\n",
    "\n",
    "print(\"Waiting for TCP connection...\")\n",
    "conn, addr = s.accept()\n",
    "\n",
    "print(\"Connected... Starting getting tweets.\")\n",
    "resp = get_tweets()\n",
    "send_tweets_to_spark(resp, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Building Spark Streaming Application\n",
    "\n",
    "This program will do real-time processing for the tweets that are streamed and help us to do the analysis.\n",
    "\n",
    "In this program we are going to create Streaming Context ssc from sc sparkContext with a batch interval ten seconds that will do the transformation on all streams received every ten seconds. We defined a checkpoint here in order to allow periodic RDD checkpointing; this is mandatory to be used in our app, as we’ll use stateful transformations.Then we define our main DStream dataStream that will connect to the socket server we created before on port and read the tweets from that port. Each record in the DStream will be a tweet.\n",
    "\n",
    "Let’s create a file called stream_app.py with the following code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'requests'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\chai8\\Documents\\Self-taught-Python\\CSGroup\\stream_spark.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/chai8/Documents/Self-taught-Python/CSGroup/stream_spark.ipynb#ch0000003?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/chai8/Documents/Self-taught-Python/CSGroup/stream_spark.ipynb#ch0000003?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrequests\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/chai8/Documents/Self-taught-Python/CSGroup/stream_spark.ipynb#ch0000003?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m \u001b[39mimport\u001b[39;00m SparkConf,SparkContext\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/chai8/Documents/Self-taught-Python/CSGroup/stream_spark.ipynb#ch0000003?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstreaming\u001b[39;00m \u001b[39mimport\u001b[39;00m StreamingContext\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'requests'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import requests\n",
    "\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row, SQLContext\n",
    "\n",
    "def aggregate_tags_count(new_values, total_sum):\n",
    "  return sum(new_values) + (total_sum or 0)\n",
    "\n",
    "def get_sql_context_instance(spark_context):\n",
    "  if ('sqlContextSingletonInstance' not in globals()):\n",
    "          globals()['sqlContextSingletonInstance'] = SQLContext(spark_context)\n",
    "  return globals()['sqlContextSingletonInstance']\n",
    "\n",
    "\n",
    "\n",
    "def process_rdd(time, rdd):\n",
    "  print(\"----------- %s -----------\" % str(time))\n",
    "  try:\n",
    "    # Get spark sql singleton context from the current context\n",
    "    sql_context = get_sql_context_instance(rdd.context)\n",
    "    print(\"Get spark sql singleton context from the current context ----------- %s -----------\" % str(time))\n",
    "\n",
    "    # convert the RDD to Row RDD\n",
    "    row_rdd = rdd.map(lambda w: Row(word=w[0], word_count=w[1]))\n",
    "\n",
    "    # create a DF from the Row RDD\n",
    "    hashtags_df = sql_context.createDataFrame(row_rdd)\n",
    "    \n",
    "    # Register the dataframe as table\n",
    "    hashtags_df.registerTempTable(\"hashtags\")\n",
    "\n",
    "    # get the top 10 hashtags from the table using SQL and print them\n",
    "    hashtag_counts_df = sql_context.sql(\"select word , word_count from hashtags where word like '#%'order by word_count desc limit 10\")\n",
    "    hashtag_counts_df.show()\n",
    "    hashtag_counts_df.coalesce(1).write.format('com.databricks.spark.csv').mode('overwrite').option(\"header\", \"true\").csv(\"/Users/girishdurgaiah/hashtag_file.csv\") \n",
    "\n",
    "    country_counts_df = sql_context.sql(\"select word as country_code, word_count as tweet_count from hashtags where word like 'CC%'order by word_count desc limit 10\")\n",
    "    country_counts_df.show()\n",
    "    country_counts_df.coalesce(1).write.format('com.databricks.spark.csv').mode('overwrite').option(\"header\", \"true\").csv(\"/Users/girishdurgaiah/country_file.csv\")\n",
    "\n",
    "    device_df = sql_context.sql(\"select word as device, word_count as device_count from hashtags where word like 'TS%'order by word_count desc limit 10\")\n",
    "    device_df.show()\n",
    "    device_df.coalesce(1).write.format('com.databricks.spark.csv').mode('overwrite').option(\"header\", \"true\").csv(\"/Users/girishdurgaiah/device_file.csv\")\n",
    "          \n",
    "  except:\n",
    "      pass\n",
    "\n",
    "# create spark configuration\n",
    "conf = SparkConf()\n",
    "conf.setAppName(\"TwitterStreamApp\")\n",
    "\n",
    "# create spark context with the above configuration\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# create the Streaming Context from the above spark context with interval size 2 seconds\n",
    "ssc = StreamingContext(sc, 2)\n",
    "\n",
    "# setting a checkpoint to allow RDD recovery\n",
    "ssc.checkpoint(\"checkpoint_TwitterApp\")\n",
    "\n",
    "# read data from port 9009\n",
    "dataStream = ssc.socketTextStream(\"localhost\",9009)\n",
    "\n",
    "\n",
    "# split each tweet into words\n",
    "words = dataStream.flatMap(lambda line: line.split(\" \"))\n",
    "              \n",
    "# filter the words to get only hashtags, then map each hashtag to be a pair of (hashtag,1)\n",
    "hashtags = words.map(lambda x: (x, 1)) \n",
    "\n",
    "# adding the count of each hashtag to its last count\n",
    "tags_totals = hashtags.updateStateByKey(aggregate_tags_count)\n",
    "\n",
    "# do processing for each RDD generated in each interval\n",
    "tags_totals.foreachRDD(process_rdd)\n",
    "\n",
    "\n",
    "# start the streaming computation\n",
    "ssc.start()\n",
    "\n",
    "# wait for the streaming to finish\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Visualization of the Twitter Data\n",
    "\n",
    "This program will display the graphs for the varous analysis we did with the twitter streaming data.\n",
    "\n",
    "Let’s create a file called visual_app.py with the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/hashtag.csv')\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "objects = df.word\n",
    "y_pos = np.arange(len(objects))\n",
    "count = df.word_count\n",
    " \n",
    "plt.barh(y_pos, count, align='center', alpha=0.5, color=\"navy\")\n",
    "plt.yticks(y_pos, objects)\n",
    "plt.xlabel('Count')\n",
    "plt.title('Top Ten Hashtag Tweets')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/country.csv')\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = df.country_code\n",
    "y_pos = np.arange(len(objects))\n",
    "count = df.tweet_count\n",
    " \n",
    "plt.barh(y_pos, count, align='center', alpha=0.5, color=\"brown\")\n",
    "plt.yticks(y_pos, objects)\n",
    "plt.xlabel('Count')\n",
    "plt.title('Tweets from Top 10 countries')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/device.csv')\n",
    "\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = df.device\n",
    "y_pos = np.arange(len(objects))\n",
    "count = df.device_count\n",
    " \n",
    "plt.barh(y_pos, count, align='center', alpha=0.5, color=\"yellow\")\n",
    "plt.yticks(y_pos, objects)\n",
    "plt.xlabel('Count')\n",
    "plt.title('Tweets originating from devices')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the time I spent in learning and completing this project i learnt Twitter has rich search functionality which has many nuances is one of its best features. With a better understanding and little practice we can turn Twitter search into a powerful tool for connecting people around the world. For businesses it offers variety of targeting options like gender,language,interest,geography etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "46b813a7ed274b33fc209ca621f36503e24d559ac96357b62b25104864439a2c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
